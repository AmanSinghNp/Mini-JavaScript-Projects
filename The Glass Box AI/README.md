# Glass Box AI

> A transparent, interactive neural network engine in vanilla JavaScript. **Visibility first, performance second.**

![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

## Mission

Unlike standard ML libraries (TensorFlow, PyTorch) which are "black boxes" optimized for speed, **Glass Box AI** is designed for **understanding**. We sacrifice raw performance for absolute visibility, allowing you to see the mathematical "thoughts" of the AI as they happen.

Think of it like opening up a mechanical watch to see the gears turningâ€”we're doing that for Artificial Intelligence.

## Features

- âœ… **No libraries**: Pure vanilla JavaScript with zero ML dependencies
- âœ… **Backpropagation from scratch**: Implements the full forward/backward pass
- âœ… **Educational JSDoc**: Every function explains the math behind it
- âœ… **Activation functions**: Sigmoid and ReLU with derivatives
- âœ… **Input validation**: Clear error messages for debugging
- âœ… **Static Visualization**: Render any network architecture
- âœ… **Real-time Training Animation**: Watch weights evolve
- âœ… **Decision Boundary Heatmap**: Visualise the 2D decision space

## Quick Start

### Installation

```bash
cd "The Glass Box AI"
npm install
```

### Run Visual Demo

```bash
npm run dev
```

Then open the URL shown (usually http://localhost:5173).
- **Start**: Begin the training loop.
- **Reset**: Re-initialize weights.
- **Architecture**: Switch between network sizes.
- **Heatmap**: Observe the learning process in 2D.

### Run Tests

```bash
npm test
```

## Architecture

```mermaid
graph TD
    UI[UI Layer] -->|Controls| Trainer[TrainerController]
    UI -->|Renders| Renderer[NetworkRenderer]
    UI -->|Renders| Heatmap[DecisionBoundary]
    
    Trainer -->|Orchestrates| Network[Core Network]
    Trainer -->|Triggers| Renderer
    Trainer -->|Triggers| Heatmap
    
    Renderer -->|Reads| Network
    Heatmap -->|Reads| Network
```

## How Backpropagation Works (In This Codebase)

### Forward Pass

Data flows from input to output. For each neuron:

```
z = dot(weights, inputs) + bias    # Pre-activation
a = activation(z)                   # Activation (output)
```

### Backward Pass

Error flows from output to input. We compute **delta** (Î´ = dLoss/dz) for each neuron:

**Output layer:**
```
Î´ = (a - target) * f'(z)
```

**Hidden layers:**
```
Î´ = (Î£ w_next * Î´_next) * f'(z)
```

### Weight Update

We use gradient descent to minimize loss:

```
weight = weight - learningRate * Î´ * input
bias = bias - learningRate * Î´
```

> **Note**: The minus sign is intentional! We move in the opposite direction of the gradient to minimize loss.

## Project Structure

```
The Glass Box AI/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/           # Neural network engine (no dependencies)
â”‚   â”‚   â”œâ”€â”€ math.js     # Vector utilities & seeded random
â”‚   â”‚   â”œâ”€â”€ Network.js  # Network orchestration
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ ui/             # Visualization Layer
â”‚   â”‚   â”œâ”€â”€ NetworkRenderer.js # Network Graph (SVG/HTML)
â”‚   â”‚   â”œâ”€â”€ DecisionBoundary.js # Heatmap (Canvas)
â”‚   â”‚   â”œâ”€â”€ TrainerController.js # Loop Logic
â”‚   â”‚   â””â”€â”€ styles.css
â”‚   â””â”€â”€ main.js         # Entry point
â”œâ”€â”€ tests/              # Vitest suite
â””â”€â”€ index.html          # Browser demo entry point
```

## Roadmap

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Headless Engine (console) | âœ… Complete |
| 2 | Static Network Rendering | âœ… Complete |
| 3 | Live Training Animation | âœ… Complete |
| 4 | Decision Boundary Heatmap | âœ… Complete |
| 5 | Final Polish | âœ… Complete |

## License

MIT
### Installation

```bash
cd "The Glass Box AI"
npm install
```

### Run Phase 2 Visual Demo

```bash
npm run dev
```

Then open the URL shown (usually http://localhost:5173).
- Use the dropdown to switch between architectures (e.g. 2,2,1 vs 2,3,1).
- Hover over connections to see weight values.
- **Note**: This is a static visualization. Training loop is not yet connected.

### Run XOR Console Demo

```bash
npm run demo
```

This trains a `[2, 2, 1]` network to solve the XOR problem. You'll see:
- Loss decreasing over epochs
- Final predictions for all 4 XOR inputs
- Convergence verification

### Run Tests

```bash
npm test
```

## Project Structure

```
The Glass Box AI/
â”œâ”€â”€ src/core/           # Neural network engine (no dependencies)
â”‚   â”œâ”€â”€ activations.js  # Sigmoid, ReLU with derivatives
â”‚   â”œâ”€â”€ math.js         # Vector utilities (dot, addVec)
â”‚   â”œâ”€â”€ Neuron.js       # Single neuron with forward pass
â”‚   â”œâ”€â”€ Layer.js        # Layer with backpropagation
â”‚   â”œâ”€â”€ Network.js      # Network orchestration
â”‚   â””â”€â”€ index.js        # Public API exports
â”œâ”€â”€ src/ui/             # Visualization Layer (Phase 2)
â”‚   â”œâ”€â”€ NetworkRenderer.js # Hybrid SVG/HTML renderer
â”‚   â”œâ”€â”€ layout.js       # Pure layout logic
â”‚   â””â”€â”€ styles.css      # CSS for neurons and lines
â”œâ”€â”€ demo/
â”‚   â””â”€â”€ xor_console.js  # XOR training demonstration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ core/           # Engine tests
â”‚   â””â”€â”€ ui/             # Layout logic tests
â””â”€â”€ index.html          # Browser demo entry point
```

## How Backpropagation Works (In This Codebase)

### Forward Pass

Data flows from input to output. For each neuron:

```
z = dot(weights, inputs) + bias    # Pre-activation
a = activation(z)                   # Activation (output)
```

### Backward Pass

Error flows from output to input. We compute **delta** (Î´ = dLoss/dz) for each neuron:

**Output layer:**
```
Î´ = (a - target) * f'(z)
```

**Hidden layers:**
```
Î´ = (Î£ w_next * Î´_next) * f'(z)
```

### Weight Update

We use gradient descent to minimize loss:

```
weight = weight - learningRate * Î´ * input
bias = bias - learningRate * Î´
```

> **Note**: The minus sign is intentional! We move in the opposite direction of the gradient to minimize loss.

## API Example

```javascript
import { Network, Sigmoid } from './src/core/index.js';

// Create network
const network = new Network({
  topology: [2, 3, 1],      // 2 inputs, 3 hidden, 1 output
  learningRate: 0.5,
  hiddenActivation: Sigmoid,
  outputActivation: Sigmoid
});

// Predict
const output = network.predict([0.5, 0.8]);

// Train on single sample
const loss = network.trainStep([0, 1], [1]);

// Train on dataset
const avgLoss = network.trainEpoch([
  { input: [0, 0], target: [0] },
  { input: [0, 1], target: [1] },
  // ...
]);
```

## Roadmap

| Phase | Description | Status |
|-------|-------------|--------|
| 1 | Headless Engine (console) | âœ… Complete |
| 2 | Static Network Rendering | âœ… Complete |
| 3 | Live Training Animation | ðŸš§ Planned |
| 4 | Decision Boundary Heatmap | ðŸš§ Planned |

## License

MIT
